{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Topic Modelling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is another personal project of mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These commands have been run in the JupyterLab console:\n",
    "\n",
    "pip install praw \n",
    "\n",
    "pip install stop_words\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "pip install sklearn\n",
    "\n",
    "NOTE: We install these modules specifically using pip here because Conda version is\n",
    "outdated. A seperate Conda environment was created to avoid package management issues.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the following packages for the following praw program in the next cell\n",
    "import praw as praw\n",
    "import random  \n",
    "import socket\n",
    "import sys\n",
    "#DONE#\n",
    "\n",
    "#import the following packages for creating a corpus\n",
    "\n",
    "import string #import string module for string manipulation\n",
    "import stop_words #import base stop_words \n",
    "import nltk #import nltk for removing extra stop words and tokenising strings for text preprocessor function\n",
    "\n",
    "from stop_words import get_stop_words #(About 900 stop words)\n",
    "from nltk.corpus import stopwords #(An extra 150 stop words)\n",
    "from nltk.tokenize import word_tokenize #(tokeniser function)\n",
    "\n",
    "'''\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "Download the stopwords and punkt resource for nltk if necessary\n",
    "'''\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code taken from praw documentation to initiate program to obtain refresh token for Reddit API\n",
    "authorisation. This is necessary to avoid using personal username and password for\n",
    "authentication.\n",
    "'''\n",
    "def receive_connection():\n",
    "    \"\"\"Wait for and then return a connected socket..\n",
    "\n",
    "    Opens a TCP connection on port 8080, and waits for a single client.\n",
    "\n",
    "    \"\"\"\n",
    "    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    server.bind((\"localhost\", 8080))\n",
    "    server.listen(1)\n",
    "    client = server.accept()[0]\n",
    "    server.close()\n",
    "    return client\n",
    "\n",
    "\n",
    "def send_message(client, message):\n",
    "    \"\"\"Send message to client and close the connection.\"\"\"\n",
    "    print(message)\n",
    "    client.send(f\"HTTP/1.1 200 OK\\r\\n\\r\\n{message}\".encode(\"utf-8\"))\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Provide the program's entry point when directly executed.\"\"\"\n",
    "    print(\n",
    "        \"Go here while logged into the account you want to create a token for: \"\n",
    "        \"https://www.reddit.com/prefs/apps/\"\n",
    "    )\n",
    "    print(\n",
    "        \"Click the create an app button. Put something in the name field and select the\"\n",
    "        \" script radio button.\"\n",
    "    )\n",
    "    print(\"Put http://localhost:8080 in the redirect uri field and click create app\")\n",
    "    client_id = input(\n",
    "        \"Enter the client ID, it's the line just under Personal use script at the top: \"\n",
    "    )\n",
    "    client_secret = input(\"Enter the client secret, it's the line next to secret: \")\n",
    "    commaScopes = input(\n",
    "        \"Now enter a comma separated list of scopes, or all for all tokens: \"\n",
    "    )\n",
    "\n",
    "    if commaScopes.lower() == \"all\":\n",
    "        scopes = [\"*\"]\n",
    "    else:\n",
    "        scopes = commaScopes.strip().split(\",\")\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id.strip(),\n",
    "        client_secret=client_secret.strip(),\n",
    "        redirect_uri=\"http://localhost:8080\",\n",
    "        user_agent=\"praw_refresh_token_example\",\n",
    "    )\n",
    "    state = str(random.randint(0, 65000))\n",
    "    url = reddit.auth.url(scopes, state, \"permanent\")\n",
    "    print(f\"Now open this url in your browser: {url}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    client = receive_connection()\n",
    "    data = client.recv(1024).decode(\"utf-8\")\n",
    "    param_tokens = data.split(\" \", 2)[1].split(\"?\", 1)[1].split(\"&\")\n",
    "    params = {\n",
    "        key: value for (key, value) in [token.split(\"=\") for token in param_tokens]\n",
    "    }\n",
    "\n",
    "    if state != params[\"state\"]:\n",
    "        send_message(\n",
    "            client,\n",
    "            f\"State mismatch. Expected: {state} Received: {params['state']}\",\n",
    "        )\n",
    "        return 1\n",
    "    elif \"error\" in params:\n",
    "        send_message(client, params[\"error\"])\n",
    "        return 1\n",
    "\n",
    "    refresh_token = reddit.auth.authorize(params[\"code\"])\n",
    "    send_message(client, f\"Refresh token: {refresh_token}\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"\", \n",
    "                                   client_secret = \"\",\n",
    "                                  user_agent = \"\",\n",
    "                                  refresh_token = \"\"\n",
    "                                  )\n",
    "\n",
    "#Authentication credentials used above to get access to Reddit API\n",
    "#Insert generated refresh token from the previous cell into the empty parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit.user.me()) #check that the user login worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"\") #Set the subreddit of choice to stream Reddit post submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = subreddit.new(limit = 1000) #Create variable that gathers new submissions from subreddit, input integer to limit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = [] #Create empty list to hold submission titles aka sub_list. *change variable name if required*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that extracts submission titles from Reddit API and appends into a list.\n",
    "\n",
    "def streamer():\n",
    "\n",
    "    for submission in x:\n",
    "        sub_list.append(submission.title)\n",
    "        print(submission.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer() #call streamer function to create list containing submission titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + get_stop_words('en'))#create stop words object from nltk stopwords resource\n",
    "\n",
    "punc = \".?=+%()-_|/[]!:;@<>&^*!'',#\" #variable for punctuation to be omitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to prepare text for language processing\n",
    "\n",
    "#list_2_str is the list that is to be converted into a string\n",
    "\n",
    "def string_preprocessor(list_2_str):\n",
    "    \n",
    "        list_2_str = str(list_2_str) #convert list into string format\n",
    "\n",
    "        list_2_str = list_2_str.lower() #change all text in string to lowercase format\n",
    "        \n",
    "        list_2_str = list_2_str.strip() #remove all whitespace from string\n",
    "\n",
    "        for punctuation in list_2_str:\n",
    "            if punctuation in punc:\n",
    "                list_2_str = list_2_str.replace(punctuation, \"\") #remove punctuation from string\n",
    "        \n",
    "        return(list_2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = string_preprocessor(sub_list) #input your list to be converted to string here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = word_tokenize(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = [word for word in sub_tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = [word for word in sub_tokens if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sub_tokens\n",
    "\n",
    "del(sub_tokens)\n",
    "del(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will write a text document containing all the terms taken from the subreddit.\n",
    "str_corpus = str(corpus)\n",
    "\n",
    "corpus_text = open(\"sample.txt\", \"w\") #change sample.txt to file name for text file.\n",
    "corpus_text.write(str_corpus)\n",
    "corpus_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above extracts textual data from subreddits on Reddit. It then preprocesses the data by removing stop words, punctuation and converting to all lower case. This data is then converted into a text document, which will now be used to create a document-term matrix to be used for probabilistic topic modelling, using sklearn and gensim. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
