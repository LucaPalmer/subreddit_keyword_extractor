{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit Topic Modelling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is another personal project of mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThese commands have been run in the JupyterLab console:\\n\\npip install praw \\n\\npip install stop_words\\n\\npip install nltk\\n\\npip install sklearn\\n\\nNOTE: We install these modules specifically using pip here because Conda version is\\noutdated. A seperate Conda environment was created to avoid package management issues.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "These commands have been run in the JupyterLab console:\n",
    "\n",
    "pip install praw \n",
    "\n",
    "pip install stop_words\n",
    "\n",
    "pip install nltk\n",
    "\n",
    "pip install sklearn\n",
    "\n",
    "NOTE: We install these modules specifically using pip here because Conda version is\n",
    "outdated. A seperate Conda environment was created to avoid package management issues.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\\nDownload the stopwords and punkt resource for nltk if necessary\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the following packages for the following praw program in the next cell\n",
    "import praw as praw\n",
    "import random  \n",
    "import socket\n",
    "import sys\n",
    "#DONE#\n",
    "\n",
    "#import the following packages for creating a corpus\n",
    "\n",
    "import string #import string module for string manipulation\n",
    "import stop_words #import base stop_words \n",
    "import nltk #import nltk for removing extra stop words and tokenising strings for text preprocessor function\n",
    "\n",
    "from stop_words import get_stop_words #(About 900 stop words)\n",
    "from nltk.corpus import stopwords #(An extra 150 stop words)\n",
    "from nltk.tokenize import word_tokenize #(tokeniser function)\n",
    "\n",
    "'''\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "Download the stopwords and punkt resource for nltk if necessary\n",
    "'''\n",
    "#DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go here while logged into the account you want to create a token for: https://www.reddit.com/prefs/apps/\n",
      "Click the create an app button. Put something in the name field and select the script radio button.\n",
      "Put http://localhost:8080 in the redirect uri field and click create app\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the client ID, it's the line just under Personal use script at the top:  93b74cYiSei-YQ\n",
      "Enter the client secret, it's the line next to secret:  uUqBxVmcwVAqsYZ-fov-3v4srA3QuA\n",
      "Now enter a comma separated list of scopes, or all for all tokens:  all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now open this url in your browser: https://www.reddit.com/api/v1/authorize?client_id=93b74cYiSei-YQ&duration=permanent&redirect_uri=http%3A%2F%2Flocalhost%3A8080&response_type=code&scope=%2A&state=42128\n",
      "Refresh token: 690585682570-gOwWT7YE6jPQe4SwjBmDmrw7n8r9oQ\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code taken from praw documentation to initiate program to obtain refresh token for Reddit API\n",
    "authorisation. This is necessary to avoid using personal username and password for\n",
    "authentication.\n",
    "'''\n",
    "def receive_connection():\n",
    "    \"\"\"Wait for and then return a connected socket..\n",
    "\n",
    "    Opens a TCP connection on port 8080, and waits for a single client.\n",
    "\n",
    "    \"\"\"\n",
    "    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    server.bind((\"localhost\", 8080))\n",
    "    server.listen(1)\n",
    "    client = server.accept()[0]\n",
    "    server.close()\n",
    "    return client\n",
    "\n",
    "\n",
    "def send_message(client, message):\n",
    "    \"\"\"Send message to client and close the connection.\"\"\"\n",
    "    print(message)\n",
    "    client.send(f\"HTTP/1.1 200 OK\\r\\n\\r\\n{message}\".encode(\"utf-8\"))\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Provide the program's entry point when directly executed.\"\"\"\n",
    "    print(\n",
    "        \"Go here while logged into the account you want to create a token for: \"\n",
    "        \"https://www.reddit.com/prefs/apps/\"\n",
    "    )\n",
    "    print(\n",
    "        \"Click the create an app button. Put something in the name field and select the\"\n",
    "        \" script radio button.\"\n",
    "    )\n",
    "    print(\"Put http://localhost:8080 in the redirect uri field and click create app\")\n",
    "    client_id = input(\n",
    "        \"Enter the client ID, it's the line just under Personal use script at the top: \"\n",
    "    )\n",
    "    client_secret = input(\"Enter the client secret, it's the line next to secret: \")\n",
    "    commaScopes = input(\n",
    "        \"Now enter a comma separated list of scopes, or all for all tokens: \"\n",
    "    )\n",
    "\n",
    "    if commaScopes.lower() == \"all\":\n",
    "        scopes = [\"*\"]\n",
    "    else:\n",
    "        scopes = commaScopes.strip().split(\",\")\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id.strip(),\n",
    "        client_secret=client_secret.strip(),\n",
    "        redirect_uri=\"http://localhost:8080\",\n",
    "        user_agent=\"praw_refresh_token_example\",\n",
    "    )\n",
    "    state = str(random.randint(0, 65000))\n",
    "    url = reddit.auth.url(scopes, state, \"permanent\")\n",
    "    print(f\"Now open this url in your browser: {url}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    client = receive_connection()\n",
    "    data = client.recv(1024).decode(\"utf-8\")\n",
    "    param_tokens = data.split(\" \", 2)[1].split(\"?\", 1)[1].split(\"&\")\n",
    "    params = {\n",
    "        key: value for (key, value) in [token.split(\"=\") for token in param_tokens]\n",
    "    }\n",
    "\n",
    "    if state != params[\"state\"]:\n",
    "        send_message(\n",
    "            client,\n",
    "            f\"State mismatch. Expected: {state} Received: {params['state']}\",\n",
    "        )\n",
    "        return 1\n",
    "    elif \"error\" in params:\n",
    "        send_message(client, params[\"error\"])\n",
    "        return 1\n",
    "\n",
    "    refresh_token = reddit.auth.authorize(params[\"code\"])\n",
    "    send_message(client, f\"Refresh token: {refresh_token}\")\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = \"\", \n",
    "                                   client_secret = \"\",\n",
    "                                  user_agent = \"\",\n",
    "                                  refresh_token = \"\"\n",
    "                                  )\n",
    "\n",
    "#Authentication credentials used above to get access to Reddit API\n",
    "#Insert generated refresh token from the previous cell into the empty parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit.user.me()) #check that the user login worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"\") #Set the subreddit of choice to stream Reddit post submissions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = subreddit.new(limit = 1000) #Create variable that gathers new submissions from subreddit, input integer to limit posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = [] #Create empty list to hold submission titles aka sub_list. *change variable name if required*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function that extracts submission titles from Reddit API and appends into a list.\n",
    "\n",
    "def streamer():\n",
    "\n",
    "    for submission in x:\n",
    "        sub_list.append(submission.title)\n",
    "        print(submission.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mississippi spent welfare money in ‘absurd’ ways, but new bill would up amount for families\n",
      "Biden signs orders on migrant family separations and asylum\n",
      "Biden Admin will send Covid Vaccines directly to Pharmacies\n",
      "SpaceX Mars rocket prototype explodes on landing, again\n",
      "Jeff Bezos to step down as Amazon CEO, Andy Jassy to take over in Q3\n",
      "Ancient mummies with golden tongues unearthed in Egypt\n",
      "People in Myanmar honk horns, bang on pots to protest coup\n",
      "Tesla recalls 135,000 cars after pushing back against regulators\n",
      "5 children, 1 adult killed in Oklahoma shooting\n",
      "Whole Foods must face lawsuit over its honey graham crackers\n",
      "Iowa man pleads guilty to BWI crash that claimed life of local race car driver\n",
      "Suspected Chinese hackers used SolarWinds bug to spy on U.S. payroll agency\n",
      "Bangladesh bought mass spying equipment from Israeli company\n",
      "Senate confirms Pete Buttigieg as transportation secretary\n",
      "India: Journalists Covering Farmer Protests Charged\n",
      "Ex-Nebraska clerk praised for tornado work gets prison term\n",
      "No prison time for Maryland Capital Crescent Trail attacker\n",
      "Oxford/Astrazeneca coronavirus vaccine shows sustained protection of 76% after First Dose during the 3-month interval until the second dose\n",
      "Andrew Yang tests positive for Covid-19\n",
      "Angelina Jolie is selling Winston Churchill's only painting created during World War II\n",
      "Navalny Sentenced to 2.5 Years in Penal Colony\n",
      "Yale is offering its highly popular 'happiness' course to low-income US high school students for free\n",
      "Robinhood raises trading limits on restricted stocks, customers can buy 100 GameStop shares now\n",
      "Boeing giving employee bonuses despite losing $12B last year\n",
      "Amazon to pay $61.7 million to settle charges it stole some driver tips\n",
      "Captain Sir Tom Moore dies at 100 after testing positive for Covid\n",
      "EU shellfish import ban permanent, UK fishing industry told\n",
      "Twitter suspends hundreds of Indian accounts after government demand\n",
      "Brexit rules mean 15m baby bees may be seized and burned, says beekeeper\n",
      "Teen in state custody sexually assaulted social worker in hotel, union leader says\n",
      "Several FBI agents shot while serving warrant at Florida home\n",
      "Uber to buy alcohol delivery service Drizly for $1.1 billion\n",
      "Amsterdam to move sex workers out of city centre in tourism ‘reset’\n",
      "PS5 Restock in Japan Canceled After Crowd Violence\n",
      "5th generation owner of Welland farm fighting city's expropriation for industrial park\n",
      "84-year-old killed after horrific daytime attack caught on video in San Francisco\n",
      "Oakland's Chinatown on edge after more than 20 reported robberies, Chamber of Commerce president says\n",
      "Passenger At Oakland Airport Issued Fake Bomb Threat Because He Was ‘Upset’ By Flight Delay\n",
      "UK variant has mutated again, scientists say\n",
      "Pfizer expects $15bn sales of Covid-19 vaccine\n",
      "Exxon posts first annual loss as a public company on COVID-19 blow\n",
      "Future of Holocaust research in Poland hinges on libel case\n",
      "Hal Holbrook, Actor Who Channeled Mark Twain, Is Dead at 95\n",
      "Facebook tries to remind users about benefits of data collection ahead of Apple privacy change\n",
      "Russia's Sputnik V vaccine is 91.6% effective against symptomatic Covid-19, interim trial results suggest.\n",
      "T-shirt latest source of tension in China-Canada ties\n",
      "Minneapolis police officers must keep body cameras turned on during entire response to a call, new policy says\n",
      "Punxsutawney Phil predicts 6 more weeks of winter after seeing his shadow at virtual ceremony\n",
      "Teenager emerges after 10-month coma with no knowledge of pandemic\n",
      "California man falls to his death while canyoneering at Death Valley\n",
      "Moscow court hears case for jailing Putin critic Navalny\n",
      "MLB Players Reject Delay, Vow to Start Season on Time\n",
      "Myanmar activist group launches civil disobedience campaign\n",
      "Economics’ failure over destruction of nature presents ‘extreme risks’\n",
      "K-pop star apologises over 'Nazi mannequin' image\n",
      "Exercise instructor appears to unwittingly capture Myanmar coup in dance video\n",
      "Chinese state newspaper omits Jack Ma from list of entrepreneurial leaders\n",
      "Rhino poaching in South Africa falls during Covid-19 lockdown\n",
      "Six people are in custody after changing the Hollywood sign to read \"Hollyboob.\"\n",
      "Myanmar generals tighten grip on power as U.S. threatens sanctions\n",
      "Israel to give 5,000 coronavirus vaccines to Palestinian doctors\n",
      "Woman arrested for shooting Phoenix store worker while trying to stop robbers, police say\n",
      "FEMA asks Pentagon to ready as many as 10,000 troops to support nationwide vaccine push\n",
      "DC police investigating person who offered to sell COVID-19 vaccine\n",
      "Ocasio-Cortez Says She Is a Sexual Assault Survivor\n",
      "A college student made big bucks off GameStop stock. Now he's donating video games to a children's hospital\n",
      "Kroger to Close Long Beach Ralphs' and Food-4-Less locations after City Inacts “Hero Pay” Mandate\n",
      "Man wanted in Carlos Ghosn's escape accuses US of 'betrayal'\n",
      "Atlanta rapper Silentó charged with murdering cousin\n",
      "India: On Camera, Indore Municipal Workers Caught Dumping Homeless Outside City\n",
      "A Minnesota Case Marks the First Detection of the Brazil Coronavirus Variant in the U.S.\n",
      "WallStreetBets says Reddit group hit by \"large amount\" of bot activity\n",
      "Indian police arrest, investigate journalists covering farmers’ protests\n",
      "Mississippi deputy fatally shot while responding to call\n",
      "Famed Investigator Jack Palladino Dies From Attempted Mugging Injuries\n",
      "Man visited ER 3 times before dying in Missouri hospital parking lot, family says\n",
      "Dog of man who was missing in Queensland bush found alive after three weeks\n",
      "North Carolina DMV removes Confederate battle flag license plate\n",
      "Investigators recommend no charges for US Capitol Police officer accused of killing pro-Trump rioter during insurrection\n",
      "159 Bogaziçi University students detained for protesting Erdogan-appointed rector\n",
      "Google to shut down internal Stadia game development studios\n",
      "Rochester Mayor Suspends Police Officers Who Pepper-Sprayed 9-Year-Old Girl\n",
      "Michigan zoo announces birth of 2 polar bear cubs for first time in decades\n",
      "CDC data: Majority of employees at skilled nursing facilities not receiving COVID-19 vaccine\n",
      "SpaceX Announces First Mission to Space with All-Civilian Crew - Includes Sweepstakes for a Seat\n",
      "Officer reprimanded over 'Forrest Gump' family costume with child in blackface\n",
      "Anti-vaxxers briefly shut down Dodger Stadium vaccination site over weekend, police say\n"
     ]
    }
   ],
   "source": [
    "streamer() #call streamer function to create list containing submission titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + get_stop_words('en'))#create stop words object from nltk stopwords resource\n",
    "\n",
    "punc = \".?=+%()-_|/[]!:;@<>&^*!'',#\" #variable for punctuation to be omitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to prepare text for language processing\n",
    "\n",
    "#list_2_str is the list that is to be converted into a string\n",
    "\n",
    "def string_preprocessor(list_2_str):\n",
    "    \n",
    "        list_2_str = str(list_2_str) #convert list into string format\n",
    "\n",
    "        list_2_str = list_2_str.lower() #change all text in string to lowercase format\n",
    "        \n",
    "        list_2_str = list_2_str.strip() #remove all whitespace from string\n",
    "\n",
    "        for punctuation in list_2_str:\n",
    "            if punctuation in punc:\n",
    "                list_2_str = list_2_str.replace(punctuation, \"\") #remove punctuation from string\n",
    "        \n",
    "        return(list_2_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list = string_preprocessor(sub_list) #input your list to be converted to string here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = word_tokenize(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = [word for word in sub_tokens if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tokens = [word for word in sub_tokens if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sub_tokens\n",
    "\n",
    "del(sub_tokens)\n",
    "del(sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will write a text document containing all the terms taken from the subreddit.\n",
    "str_corpus = str(corpus)\n",
    "\n",
    "corpus_text = open(\"sample.txt\", \"w\") #change sample.txt to file name for text file.\n",
    "corpus_text.write(str_corpus)\n",
    "corpus_text.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above extracts textual data from subreddits on Reddit. It then preprocesses the data by removing stop words, punctuation and converting to all lower case. This data is then converted into a text document, which will now be used to create a document-term matrix to be used for probabilistic topic modelling, using sklearn and gensim. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
